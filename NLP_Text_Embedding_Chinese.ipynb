{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alisonyang/data-science-blog/blob/main/NLP_Text_Embedding_Chinese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGATMXZE4oAD"
   },
   "source": [
    "# Training a Binary Classifier Using Chinese-Language Movie Reviews\n",
    "\n",
    "You will build a simple binary classification model to distinguish between positive and negative movie reviews, trained on the [豆瓣 movies short reviews](https://www.kaggle.com/datasets/utmhikari/doubanmovieshortcomments) dataset. The main goal of the project is to visualize embeddings produced by the model for Chinese language text using Tensorflow Embedding Projector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWt8QlOGpy1j"
   },
   "source": [
    "## Download the Dataset\n",
    "\n",
    "First, you will need to download the dataset. This data source comes from Kaggle, and to fetch Kaggle data, please refer to the following: ['Easiest way to download kaggle data in Google Colab'](https://www.kaggle.com/general/74235)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hhPUqcwwetch"
   },
   "outputs": [],
   "source": [
    "# ! pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "ermziQ5CfeXy",
    "outputId": "81eebfae-0753-4e1c-d576-31ec9b8cb2d2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MffdKpcaZ-qy",
    "outputId": "f3ab080d-9ec4-4f6e-fc86-50047ba8d957"
   },
   "outputs": [],
   "source": [
    "# ! mkdir ~/.kaggle\n",
    "# ! cp kaggle.json ~/.kaggle/\n",
    "# ! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHLSZAamekML",
    "outputId": "afe73895-c62d-4c4e-f225-8cc4d955b036"
   },
   "outputs": [],
   "source": [
    "# ! kaggle datasets list\n",
    "!kaggle datasets download -d utmhikari/doubanmovieshortcomments --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpA1w42ef-PQ"
   },
   "outputs": [],
   "source": [
    "# ! mkdir comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHgDz3XZgD9H",
    "outputId": "0deaba58-30ed-4d6a-ba2f-f3a7140b0417"
   },
   "outputs": [],
   "source": [
    "! unzip -o doubanmovieshortcomments.zip -d comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpLsMxO2wDrn"
   },
   "source": [
    "## Check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "_IoM4VFxWpMR",
    "outputId": "91264ba5-3765-402a-8505-409f00740ee9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments = pd.read_csv('/content/comments/DMSC.csv')\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3PEarpKw9_j",
    "outputId": "fdf82773-2971-4391-d168-cdd489f89837"
   },
   "outputs": [],
   "source": [
    "# the dataset information\n",
    "comments.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F14DHgllr79T",
    "outputId": "530dd2c9-8e65-48f1-9471-0a113630622c"
   },
   "outputs": [],
   "source": [
    "comments['Star'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLRAoHil5poj"
   },
   "source": [
    "As shown in the output above, the dataset contains a total of **2,125,056** reviews. For this project, we will only select the **Star**  and **Comment** columns to train our mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EzNDkdkpvrv"
   },
   "source": [
    "## Extracting Labels from the 'Star' Column\n",
    "\n",
    "In this project, we will simply transform the star rating into two labels:\n",
    "\n",
    "*   1-3 stars: negative label (**0**)\n",
    "*   4-5 stars: positive label (**1**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXQAGmLgulkv",
    "outputId": "a4ac9426-cc71-4c6a-eb98-38976b082acc"
   },
   "outputs": [],
   "source": [
    "comments[\"Star\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeg4CRQwsYA1",
    "outputId": "b5ea003a-7931-4bd9-a80f-49cf6936166a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "comments[\"Star\"] = np.where(comments[\"Star\"] > 3, 1, 0)\n",
    "comments[\"Star\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZGuc4kUuZGm",
    "outputId": "a3fa2aa2-8ca3-4a9c-ed6f-2f47ce1b82c4"
   },
   "outputs": [],
   "source": [
    "labels = comments['Star']\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB7Ekr8grPSb"
   },
   "source": [
    "## Split the dataset\n",
    "\n",
    "We will split the dataset into a training set and a testing set in a 7:3 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwHNVXI2rLNW",
    "outputId": "391ed0ee-d058-4a70-8ccd-d803b64bbbff"
   },
   "outputs": [],
   "source": [
    "reviews = comments['Comment']\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eoiDB8Wvfrp"
   },
   "outputs": [],
   "source": [
    "training_size = 1500000\n",
    "\n",
    "# Split the sentences\n",
    "training_reviews = reviews[0:training_size].tolist()\n",
    "testing_reviews = reviews[training_size:].tolist()\n",
    "\n",
    "# Split the labels\n",
    "training_labels = labels[0:training_size].tolist()\n",
    "testing_labels = labels[training_size:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42lDtAFZwFIL",
    "outputId": "4f80f1e2-fe5c-4d50-9a4d-926d8174154a"
   },
   "outputs": [],
   "source": [
    "print(\"training set length:\", len(training_reviews))\n",
    "print(\"testing_labels set length:\", len(testing_reviews))\n",
    "print(type(training_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePTIgXj3q8Sg"
   },
   "source": [
    "## Preprocessing the train and test sets\n",
    "\n",
    "Now you can preprocess the text and labels so it can be consumed by the model. \n",
    "\n",
    "1.   we will use Jieba to segment this Chinese text data.\n",
    "2.   we will create the vocabulary using the `Tokenizer` class and generate padded token sequences using the `pad_sequences` method.\n",
    "3.   we need to convert the labels into a Numpy array to ensure a valid data type for `model.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-E9QamglfeM",
    "outputId": "d57a97b5-7808-430d-80d0-04455888181c"
   },
   "outputs": [],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJ3ucCkrlkIF"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# from ckiptagger import WS\n",
    "def jieba_cut(text):\n",
    "  result = jieba.lcut(text, cut_all = False)\n",
    "\n",
    "  # For the stop word list, please refer to this link: https://github.com/goto456/stopwords/blob/master/cn_stopwords.txt.\n",
    "  stopword = open(\"cn_stopwords.txt\", \"r\", encoding='UTF-8').read()\n",
    "  stopword_list = stopword.split(\"\\n\")\n",
    "\n",
    "  seg_result = []\n",
    "\n",
    "  for word in result:\n",
    "    if word not in stopword_list:\n",
    "      seg_result.append(word)\n",
    "\n",
    "  return \" \".join(seg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36DfhZE4fU6G"
   },
   "outputs": [],
   "source": [
    "# use Jieba to segment the training and testing sets\n",
    "\n",
    "traning_set = []\n",
    "for review  in training_reviews:\n",
    "  traning_set.append(jieba_cut(review))\n",
    "\n",
    "testing_set = []\n",
    "for review  in testing_reviews:\n",
    "  testing_set.append(jieba_cut(review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtHnB1S3gH5q",
    "outputId": "c00acb24-f3d3-4284-bac0-83a132cbfacb"
   },
   "outputs": [],
   "source": [
    "traning_set[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoDAyo8Zx9Hq",
    "outputId": "82aa1cbe-cc78-484b-8205-40cf5966a37e"
   },
   "outputs": [],
   "source": [
    "testing_set[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "766MLbEcxe3z"
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "vocab_size = 10000\n",
    "max_length = 64\n",
    "embedding_dim = 16\n",
    "\n",
    "# for padding and OOV tokens\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lggoZqYUGYgX"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(traning_set)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# pad the training sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(traning_set)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# pad the testing sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_set)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# convert the labels into numpy arrays\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2rCmp7ArGL_"
   },
   "source": [
    "## Build and Compile the Model\n",
    "\n",
    "Now that the data has been preprocessed, we can move on to building our binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NEpdhb8AxID",
    "outputId": "b9748443-fc7f-46f5-b05b-ecda37780458"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8gbnoRdqp8O"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "The next step is to train our model. However, since the primary objective of this Jupyter Notebook is to visualize the text embedding information using the TensorFlow Embedding Projector, we won't be dedicating a significant amount of effort to training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5LLrXC-uNX6",
    "outputId": "443cc0d7-2c11-4c78-e019-54fa1aaf6cc4"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "IKi_j14B_T8D",
    "outputId": "35f82ee1-347b-46cd-9597-1cde2ffe37e8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "# Plot the accuracy and loss\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mroDvjEJqwm4"
   },
   "source": [
    "## Visualize Word Embeddings\n",
    "\n",
    "After the model is trained, we can visualize the weights in the Embedding layer to observe how similar words are clustered together. To achieve this, we will use the [Tensorflow Embedding Projector](https://projector.tensorflow.org/)  to reduce the 16-dimensional vectors we defined earlier into fewer components that can be plotted in the projector. In order to obtain these weights, we can execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAmjJqEyCOF_",
    "outputId": "640f0a9c-886d-4040-8570-2b4b0cb69c32"
   },
   "outputs": [],
   "source": [
    "# use reverse_word_index to lookup a word \n",
    "reverse_word_index = tokenizer.index_word\n",
    "\n",
    "# Get the embedding layer from the model\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the weights \n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "print(embedding_weights.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJGdRnEO_c9q"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Open writeable files\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for word_num in range(1, vocab_size):\n",
    "  word_name = reverse_word_index[word_num]\n",
    "  word_embedding = embedding_weights[word_num]\n",
    "\n",
    "  if len(word_name.strip()) != 0:\n",
    "    # print(word_name)\n",
    "    out_m.write(word_name + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "pmQvjSVa_dHi",
    "outputId": "58368701-cb55-4c6b-94ec-a206e344fd80"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "\n",
    "# Download the files\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEuG9AqIuF6i"
   },
   "source": [
    "We can go to the [Tensorflow Embedding Projector](https://projector.tensorflow.org/) and load the two files:\n",
    "* `vecs.tsv` - contains the vector weights of each word in the vocabulary\n",
    "* `meta.tsv` - contains the words name in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRV8ag3nyAOb"
   },
   "source": [
    "You can try to search for words like `棒呆` and `大失所望` and see what other words are closely related to them. This could be a fun and engaging way to expand our vocabulary and explore the relationships between different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YSnYTisJKM4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (std)",
   "language": "python",
   "name": "std"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
